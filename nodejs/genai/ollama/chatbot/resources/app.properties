aiModel=llama3-groq-tool-use:8b
debug=true
maxTokens=auto
maxChatSteps=40
port=5000
# Temperature: This parameter controls the randomness of the output. 
# A lower temperature (closer to 0) makes the output more 
# deterministic and focused, while a higher temperature (up to 1) 
# allows for more creativity and variability.
temperature=1
# Top-p (Nucleus Sampling): This parameter controls the diversity
# of the output by considering only the top p probability mass. 
# A lower value makes the model consider fewer options, leading 
# to more focused responses.
top_p=1
